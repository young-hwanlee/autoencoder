{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "autoencoder.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNWvAJgYIj4RBNzA8vXW+AP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/young-hwanlee/autoencoder/blob/master/autoencoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gb-0Vp-ebDlc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "742ebce7-1e17-447a-9f00-c7d398680d90"
      },
      "source": [
        "!git clone https://github.com/young-hwanlee/autoencoder.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'autoencoder'...\n",
            "remote: Enumerating objects: 62, done.\u001b[K\n",
            "remote: Counting objects: 100% (62/62), done.\u001b[K\n",
            "remote: Compressing objects: 100% (59/59), done.\u001b[K\n",
            "remote: Total 62 (delta 15), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (62/62), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTSZVqEsbVfm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0841a528-cd4b-46ff-a379-dd80b45f9f26"
      },
      "source": [
        "ls -ltr"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 8\n",
            "drwxr-xr-x 1 root root 4096 Mar  5 14:37 \u001b[0m\u001b[01;34msample_data\u001b[0m/\n",
            "drwxr-xr-x 3 root root 4096 Mar 16 05:13 \u001b[01;34mautoencoder\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5RtVH7jbVnd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUUX7e0MbVqh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yeofNlRbVt9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfq5mh0qbVwh"
      },
      "source": [
        "#%%\r\n",
        "import os\r\n",
        "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\r\n",
        "\r\n",
        "from tqdm import trange\r\n",
        "import tensorflow as tf\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "from tensorflow.examples.tutorials.mnist import input_data\r\n",
        "mnist = input_data.read_data_sets(\"/tmp/data/\")\r\n",
        "\r\n",
        "## Set up the seed for reproducibility.\r\n",
        "seed = 42\r\n",
        "tf.reset_default_graph()\r\n",
        "tf.set_random_seed(seed)\r\n",
        "np.random.seed(seed)\r\n",
        "\r\n",
        "## Set up the hyperparameters\r\n",
        "scale_weights = 1e-4\r\n",
        "learning_rate = 1e-1\r\n",
        "n_epochs = 50\r\n",
        "batch_size = 100\r\n",
        "\r\n",
        "#%%\r\n",
        "## [Construction phase]\r\n",
        "n_inputs = 28*28\r\n",
        "n_hidden1 = 256\r\n",
        "n_hidden2 = 256\r\n",
        "\r\n",
        "n_codes = 100\r\n",
        "n_outputs = n_inputs\r\n",
        "\r\n",
        "n_train = 20000\r\n",
        "X_train = mnist.train.images[:n_train]\r\n",
        "X_val = mnist.validation.images\r\n",
        "X_test = mnist.test.images\r\n",
        "\r\n",
        "X = tf.placeholder(tf.float32, shape=[None, n_inputs], name=\"X\")\r\n",
        "y = tf.placeholder(tf.float32, shape=[None, n_codes], name=\"y\")\r\n",
        "\r\n",
        "b1 = tf.Variable(tf.zeros(shape=[n_hidden1]), dtype=tf.float32)\r\n",
        "b2 = tf.Variable(tf.zeros(shape=[n_codes]), dtype=tf.float32)\r\n",
        "b3 = tf.Variable(tf.zeros(shape=[n_hidden2]), dtype=tf.float32)\r\n",
        "b4 = tf.Variable(tf.zeros(shape=[n_inputs]), dtype=tf.float32)\r\n",
        "\r\n",
        "## He initialization (with any variant of ReLU)\r\n",
        "initializer = tf.contrib.layers.variance_scaling_initializer(mode='FAN_AVG')\r\n",
        "W1 = tf.Variable(initializer(shape=[n_inputs, n_hidden1]), dtype=tf.float32, name='W1')\r\n",
        "W2 = tf.Variable(initializer(shape=[n_hidden1, n_codes]), dtype=tf.float32, name='W2')\r\n",
        "W3 = tf.Variable(initializer(shape=[n_codes, n_hidden2]), dtype=tf.float32, name='W3')\r\n",
        "W4 = tf.Variable(initializer(shape=[n_hidden2, n_outputs]), dtype=tf.float32, name='W4')\r\n",
        "\r\n",
        "with tf.name_scope(\"encoder\"):  # to group related nodes\r\n",
        "    hidden1 = tf.nn.relu(tf.matmul(X, W1) + b1)\r\n",
        "    codes = tf.matmul(hidden1, W2) + b2\r\n",
        "    codes_with_activation_fnc = tf.nn.relu(codes)\r\n",
        "\r\n",
        "with tf.name_scope(\"decoder\"):  # to group related nodes\r\n",
        "    hidden2 = tf.nn.relu(tf.matmul(codes_with_activation_fnc, W3) + b3)\r\n",
        "    X_prime = tf.matmul(hidden2, W4) + b4\r\n",
        "\r\n",
        "with tf.name_scope(\"loss\"):  # to group related nodes\r\n",
        "    reg_loss = tf.reduce_sum(tf.square(W1)) + tf.reduce_sum(tf.square(W2)) \\\r\n",
        "               + tf.reduce_sum(tf.square(W3)) + tf.reduce_sum(tf.square(W4))\r\n",
        "    loss = tf.add(tf.reduce_mean(tf.square(X - X_prime)), scale_weights * reg_loss, name=\"loss\")\r\n",
        "\r\n",
        "with tf.name_scope(\"train\"):  # to group related nodes\r\n",
        "    # optimizer = tf.train.GradientDescentOptimizer(learning_rate)\r\n",
        "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.9, use_nesterov=True)\r\n",
        "    training_op = optimizer.minimize(loss)\r\n",
        "\r\n",
        "#%%\r\n",
        "## [Excution phase]\r\n",
        "init = tf.global_variables_initializer()\r\n",
        "saver = tf.train.Saver()  # to save the trained model parameters to disk\r\n",
        "\r\n",
        "if cross_entropy_check == \"1\":\r\n",
        "    acc_train_hist, acc_val_hist = [], []\r\n",
        "loss_train_hist, loss_val_hist = [], []\r\n",
        "\r\n",
        "with tf.Session() as sess:\r\n",
        "    init.run()\r\n",
        "\r\n",
        "    for epoch in trange(n_epochs):\r\n",
        "        for iteration in range(int(np.shape(X_train)[0] // batch_size)):\r\n",
        "            X_batch = X_train[iteration * batch_size:(iteration + 1) * batch_size]\r\n",
        "            sess.run(training_op, feed_dict={X: X_batch})\r\n",
        "\r\n",
        "            loss_train = loss.eval(feed_dict={X: X_batch})\r\n",
        "            loss_val = loss.eval(feed_dict={X: X_val})\r\n",
        "            loss_train_hist.append(loss_train)\r\n",
        "            loss_val_hist.append(loss_val)\r\n",
        "\r\n",
        "            if iteration % 100 == 0:\r\n",
        "                print(\"\\nepoch :\", epoch, \"\\titeration :\", iteration)\r\n",
        "                print(\"loss :\", loss_val)\r\n",
        "\r\n",
        "    X_prime_check = X_prime.eval(feed_dict={X: X_val})\r\n",
        "\r\n",
        "#%%\r\n",
        "## Check the results\r\n",
        "print(\"MSE : \", np.sum(np.square(X_val - X_prime_check))/(np.shape(X_val)[0]*np.shape(X_val)[1]))\r\n",
        "\r\n",
        "plt.figure()\r\n",
        "plt.plot(loss_train_hist)\r\n",
        "plt.plot(loss_val_hist)\r\n",
        "plt.ylabel('loss')\r\n",
        "plt.xlabel('iterations')\r\n",
        "plt.legend(['train', 'validation'], loc='upper right')\r\n",
        "plt.show()\r\n",
        "\r\n",
        "#%%\r\n",
        "n = 10  # how many digits we will display\r\n",
        "plt.figure(figsize=(20, 4))\r\n",
        "for i in range(n):\r\n",
        "    # display original\r\n",
        "    ax = plt.subplot(2, n, i + 1)\r\n",
        "    plt.imshow(X_val[i].reshape(28, 28))\r\n",
        "    plt.gray()\r\n",
        "    ax.get_xaxis().set_visible(False)\r\n",
        "    ax.get_yaxis().set_visible(False)\r\n",
        "\r\n",
        "    # display reconstruction\r\n",
        "    ax = plt.subplot(2, n, i + 1 + n)\r\n",
        "    plt.imshow(X_prime_check[i].reshape(28, 28))\r\n",
        "    plt.gray()\r\n",
        "    ax.get_xaxis().set_visible(False)\r\n",
        "    ax.get_yaxis().set_visible(False)\r\n",
        "\r\n",
        "fig.text(0.475, 0.85, 'Original Figures', va='center')\r\n",
        "fig.text(0.45, 0.425, 'Reconstructed Figures', va='center')\r\n",
        "    \r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}